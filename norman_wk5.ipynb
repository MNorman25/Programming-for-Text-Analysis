{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(s):\n",
    "    return s.split()\n",
    "#originally did not use this, but got an error that said tokenize not defined, and realized this needed to be added \n",
    "#input: tokenize = needs to be defined, and also it converts the text to tokens, s.split() = splits the string and returns a list of words\n",
    "#return: Allows the file/text to be returned as a list of words that can be counted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1_king_henry_iv.txt', '1_king_henry_vi.txt', '2_king_henry_iv.txt', '2_king_henry_vi.txt', '3_king_henry_vi.txt', \"all's_well_that_ends_well.txt\", 'antony_and_cleopatra.txt', 'as_you_like_it.txt', \"a_midsummer_night's_dream.txt\", 'comedy_of_errors.txt', 'coriolanus.txt', 'cymbeline.txt', 'hamlet.txt', 'julius_caesar.txt', 'king_henry_v.txt', 'king_henry_viii.txt', 'king_john.txt', 'king_lear.txt', 'king_richard_ii.txt', 'king_richard_iii.txt', \"love's_labours_lost.txt\", 'macbeth.txt', 'measure_for_measure.txt', 'merchant_of_venice.txt', 'merry_wives_of_windsor.txt', 'much_ado_about_nothing.txt', 'othello.txt', 'romeo_and_juliet.txt', 'taming_of_the_shrew.txt', 'tempest.txt', 'timon_of_athens.txt', 'titus_andronicus.txt', 'troilus_and_cressida.txt', 'twelfth_night.txt', 'two_gentlemen_of_verona.txt', \"winter's_tale.txt\"]\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "\n",
    "path = '*.txt'\n",
    "filenames = glob.glob(path)\n",
    "print(filenames)\n",
    "#list of all the files in the shakespeare folder and these files will be used when determining the most frequent words \n",
    "#input: import = imports the text so that there is actually something to analyze, path = tells the program where the file I want to be analyzed is located\n",
    "#return: returns all the files within the shakespeare folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#next step would be to strip the punctuation and make all the words lowercase \n",
    "def preprocess(s, lowercase=True, strip_punctuation=True):\n",
    "    punctuation = '.,?<>:;\"\\'!%'\n",
    "    if isinstance(s, str):\n",
    "        s = tokenize(s)\n",
    "    if lowercase:\n",
    "        s = [t.lower() for t in s]\n",
    "    if strip_punctuation:\n",
    "        s = [t.strip(punctuation) for t in s]\n",
    "    return(s)\n",
    "#input: preprocess = allows for the dataset to be edited/converted, lowercase = makes all the strings lowercase, strip_punctuation = removes all the punctuation \n",
    "    #these inputs are essential because it allows the same word to actually be counted as the same word(example: to and To)\n",
    "#return: the punctuation and capitalization within the text is removed, so that the words are uniform \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<_io.TextIOWrapper name=\"winter's_tale.txt\" mode='r' encoding='cp1252'>\n"
     ]
    }
   ],
   "source": [
    "#this reads the files within filenames as strings\n",
    "for file in filenames:\n",
    "    f = open(file, \"r\")\n",
    "print(f)\n",
    "#input: open = opens the files in filenames\n",
    "#return: the text within the files (returns as str)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#next the words in the files should be listed according to their frequency\n",
    "def token_frequency(tokens, tf={}, relative=False):\n",
    "    for t in tokens:\n",
    "        if t in tf:\n",
    "            tf[t]+=1\n",
    "        else:\n",
    "            tf[t]=1\n",
    "    if relative:\n",
    "        tf = {k:v/len(tokens) for k, v in tf.items()}\n",
    "    return tf\n",
    "\n",
    "s = preprocess(f.read())\n",
    "d=token_frequency(s, relative=True)\n",
    "#print(d)\n",
    "\n",
    " #input:\n",
    "        #tokens = list of strings or None\n",
    "        #tf = dict or none\n",
    "        #relative = boolean\n",
    "        #f.read = reads the file as a string\n",
    "#return:\n",
    "        #dictionary of token frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#next step is to turn the words in the file into a list so that I can do the next step of listing the top 50\n",
    "a = [k for k, v in sorted(d.items(), key=lambda x: x[1], reverse=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'i', 'and', 'to', 'of', 'you', 'my', 'a', 'that', 'not', 'it', 'your', 'be', 'is', 'have', 'as', 'but', 'in', 'me', 'for', 'this', 'with', 'so', 'he', 'his', 'her', 'no', 'thou', 'him', 'by', 'which', 'what', 'if', 'will', 'sir', 'shall', 'are', 'now', 'good', 'all', 'do', 'she', 'we', 'on', 'or', 'more', 'was', 'come', 'o', 'how']\n"
     ]
    }
   ],
   "source": [
    "#now I want to list the 50 most frequently used words which is what :50 will allow me to do\n",
    "print(a[:50])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
